{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "from buffer import ReplayBuffer\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/codebase/deep-reinforcement-learning-v2/p3_collab-compet/Tennis_Linux/Tennis.x86_64\")\n",
    "\n",
    "log_path = os.getcwd()+\"/log\"\n",
    "model_dir= os.getcwd()+\"/model_dir\"\n",
    "logger = SummaryWriter(log_dir=log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_maddpg():\n",
    "    seeding()\n",
    "    # number of training episodes.\n",
    "    # change this to higher number to experiment. say 30000.\n",
    "    \n",
    "    #env = UnityEnvironment(file_name=\"/codebase/deep-reinforcement-learning-v2/p3_collab-compet/Tennis_Linux/Tennis.x86_64\")\n",
    "    # get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "\n",
    "    number_of_episodes = 5000\n",
    "    batchsize = 64\n",
    "    # how many episodes to save policy and gif\n",
    "    save_interval = 1000\n",
    "    rewards_deque = deque(maxlen=100)\n",
    "    rewards = []\n",
    "    \n",
    "    # amplitude of OU noise\n",
    "    # this slowly decreases to 0\n",
    "    noise = 1\n",
    "    noise_reduction = 0.9999\n",
    "    BUFFER_SIZE = int(1e5) # replay buffer size\n",
    "    \n",
    "    print_every = 100\n",
    "\n",
    "    parallel_envs = 0.5\n",
    "    # how many episodes before update\n",
    "    episode_per_update = 2 * parallel_envs\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    #print('states.shape', states.shape)\n",
    "    num_agents, num_spaces = states.shape\n",
    "    #print('num_agents: ', num_agents, ', num_spaces: ', num_spaces)\n",
    "        \n",
    "    #log_path = os.getcwd()+\"/log\"\n",
    "    #model_dir= os.getcwd()+\"/model_dir\"\n",
    "    \n",
    "    #os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    #torch.set_num_threads(parallel_envs)\n",
    "    #env = envs.make_parallel_env(parallel_envs)\n",
    "    \n",
    "    buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "    \n",
    "    # initialize policy and critic\n",
    "    maddpg = MADDPG(num_agents, num_spaces)\n",
    "    #logger = SummaryWriter(log_dir=log_path)\n",
    "    # training loop\n",
    "    \n",
    "    # show progressbar\n",
    "    #import progressbar as pb\n",
    "    #widget = ['episode: ', pb.Counter(),'/',str(number_of_episodes),' ', \n",
    "    #          pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ' ]\n",
    "    \n",
    "    #timer = pb.ProgressBar(widgets=widget, maxval=number_of_episodes).start()\n",
    "    \n",
    "    # use keep_awake to keep workspace from disconnecting\n",
    "    for episode in range(0, number_of_episodes):\n",
    "        rewards_this_episode = np.zeros((num_agents, ))\n",
    "        #timer.update(episode)\n",
    "\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "    \n",
    "        for agent in maddpg.maddpg_agent:\n",
    "            #print('main- reset agent noise')\n",
    "            agent.noise.reset()\n",
    "\n",
    "        episode_t = 0\n",
    "\n",
    "        while True:          \n",
    "            # explore = only explore for a certain number of episodes\n",
    "            # action input needs to be transposed\n",
    "            actions = maddpg.act(states, noise=noise)\n",
    "            #actions = maddpg.act(states, noise=0.00009)\n",
    "            noise *= noise_reduction\n",
    "\n",
    "\n",
    "            #print('main-actions', actions)\n",
    "            #actions = [a.detach().numpy() for a in actions]\n",
    "            #print('main-actions', actions)\n",
    "            #actions = np.array(actions).reshape(2, 2)\n",
    "            #actions = np.clip(actions, -1, 1)\n",
    "            #print('main-actions', actions)\n",
    "            \n",
    "            actions = torch.stack(actions).detach().numpy()\n",
    "            \n",
    "            # transpose the list of list\n",
    "            # flip the first two indices\n",
    "            # input to step requires the first index to correspond to number of parallel agents\n",
    "            #actions_for_env = np.rollaxis(actions_array, 1)\n",
    "            #print('main-actions_for_env: ', actions_for_env)\n",
    "            # step forward one frame\n",
    "            #next_states, next_states_full, rewards, dones, info = env.step(actions_for_env)\n",
    "            #env_step = env.step(actions_for_env)\n",
    "            \n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            rewards = env_info.rewards\n",
    "            next_states = env_info.vector_observations\n",
    "            dones = env_info.local_done\n",
    "            #print('main-rewards', rewards)\n",
    "\n",
    "            # add data to buffer\n",
    "            transition = (states, actions, rewards, next_states, dones)\n",
    "            buffer.push(transition)\n",
    "            \n",
    "            states = next_states\n",
    "            rewards_this_episode += rewards\n",
    "            \n",
    "\n",
    "            #print('main-len(buffer), batchsize', len(buffer), batchsize)\n",
    "            # update once after every episode_per_update\n",
    "            if len(buffer) > batchsize and episode % episode_per_update == 0:\n",
    "                for a_i in range(num_agents):\n",
    "                    samples = buffer.sample(batchsize)\n",
    "                    maddpg.update(samples, a_i, logger, noise)\n",
    "                    maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "\n",
    "            #print('main-rewards: ', rewards)\n",
    "\n",
    "            #print('rewards_this_episode: ', rewards_this_episode)\n",
    "            #print('main-np.max(rewards_this_episode): ', np.max(rewards_this_episode))\n",
    "            #print('---------------------------')\n",
    "            if np.any(dones):\n",
    "                break\n",
    "            episode_t += 1\n",
    "\n",
    "        # just get maximum rewards\n",
    "        #print('main-rewards_this_episode: ', rewards_this_episode, np.max(rewards_this_episode))\n",
    "        rewards_deque.append(np.max(rewards_this_episode))\n",
    "        average_score = np.mean(rewards_deque)\n",
    "        #print('main-rewards_deque: ', rewards_deque)\n",
    "        \n",
    "        #saving model\n",
    "        save_dict_list =[]\n",
    "        print('\\nEpisode {}\\tEpisode length: {:.2f}\\tAverage Score: {:.2f}\\tnoise: {:.2f}'.format(episode, episode_t, average_score, noise), end=\"\")\n",
    "        if episode_t % print_every == 0 or average_score > 0.5:\n",
    "            print('\\nEpisode {}\\tAverage Score: {:.2f}'.format(episode, average_score), end=\"\")\n",
    "\n",
    "            for i in range(num_agents):\n",
    "                save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                             'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                             'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                             'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "                save_dict_list.append(save_dict)\n",
    "\n",
    "                torch.save(save_dict_list, os.path.join(model_dir, 'episode-{}.pt'.format(episode)))\n",
    "\n",
    "            if average_score > 0.5:\n",
    "                break\n",
    "    env.close()\n",
    "    logger.close()\n",
    "    #timer.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Episode 0\tEpisode length: 14.00\tAverage Score: 0.00\tnoise: 1.00\n",
      "Episode 1\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 1.00\n",
      "Episode 2\tEpisode length: 18.00\tAverage Score: 0.00\tnoise: 1.00\n",
      "Episode 3\tEpisode length: 14.00\tAverage Score: 0.00\tnoise: 0.99\n",
      "Episode 4\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.99\n",
      "Episode 5\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.99\n",
      "Episode 6\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.99\n",
      "Episode 7\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.99\n",
      "Episode 8\tEpisode length: 14.00\tAverage Score: 0.00\tnoise: 0.99\n",
      "Episode 9\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.99\n",
      "Episode 10\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.98\n",
      "Episode 11\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.98\n",
      "Episode 12\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.98\n",
      "Episode 13\tEpisode length: 14.00\tAverage Score: 0.00\tnoise: 0.98\n",
      "Episode 14\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.98\n",
      "Episode 15\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.98\n",
      "Episode 16\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.98\n",
      "Episode 17\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.97\n",
      "Episode 18\tEpisode length: 14.00\tAverage Score: 0.00\tnoise: 0.97\n",
      "Episode 19\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.97\n",
      "Episode 20\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.97\n",
      "Episode 21\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.97\n",
      "Episode 22\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.97\n",
      "Episode 23\tEpisode length: 14.00\tAverage Score: 0.00\tnoise: 0.97\n",
      "Episode 24\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.96\n",
      "Episode 25\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.96\n",
      "Episode 26\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.96\n",
      "Episode 27\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.96\n",
      "Episode 28\tEpisode length: 14.00\tAverage Score: 0.00\tnoise: 0.96\n",
      "Episode 29\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.96\n",
      "Episode 30\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.96\n",
      "Episode 31\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.96\n",
      "Episode 32\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.95\n",
      "Episode 33\tEpisode length: 14.00\tAverage Score: 0.00\tnoise: 0.95\n",
      "Episode 34\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.95\n",
      "Episode 35\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.95\n",
      "Episode 36\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.95\n",
      "Episode 37\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.95\n",
      "Episode 38\tEpisode length: 14.00\tAverage Score: 0.00\tnoise: 0.95\n",
      "Episode 39\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.94\n",
      "Episode 40\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.94\n",
      "Episode 41\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.94\n",
      "Episode 42\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.94\n",
      "Episode 43\tEpisode length: 14.00\tAverage Score: 0.00\tnoise: 0.94\n",
      "Episode 44\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.94\n",
      "Episode 45\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.94\n",
      "Episode 46\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.93\n",
      "Episode 47\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.93\n",
      "Episode 48\tEpisode length: 14.00\tAverage Score: 0.00\tnoise: 0.93\n",
      "Episode 49\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.93\n",
      "Episode 50\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.93\n",
      "Episode 51\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.93\n",
      "Episode 52\tEpisode length: 13.00\tAverage Score: 0.00\tnoise: 0.93"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6f532785f915>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_maddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-d6ca42d9bd80>\u001b[0m in \u001b[0;36mmain_maddpg\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ma_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatchsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                     \u001b[0mmaddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                     \u001b[0mmaddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#soft update the target network towards the actual networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/codebase/deep-reinforcement-learning-v2/p3_collab-compet/maddpg.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, samples, agent_number, logger, noise)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mmax_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mnorm_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mparameters\u001b[0;34m(self, recurse)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \"\"\"\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecurse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse)\u001b[0m\n\u001b[1;32m    829\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m             prefix=prefix, recurse=recurse)\n\u001b[0;32m--> 831\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmemo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodule_prefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__hash__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scores = main_maddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "logger.close()\n",
    "#timer.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
