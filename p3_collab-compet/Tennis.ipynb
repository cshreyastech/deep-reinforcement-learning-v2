{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Unity_Env_Wrapper import TennisEnv\n",
    "from buffer import ReplayBuffer\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seeding(seed=1):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function that sets up environments\n",
    "# perform training loop\n",
    "\n",
    "from Unity_Env_Wrapper import TennisEnv\n",
    "from buffer import ReplayBuffer\n",
    "from maddpg import MADDPG\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import deque\n",
    "\n",
    "def main_tennis():\n",
    "    seeding()\n",
    "    # number of parallel agents\n",
    "    number_of_agents = 2\n",
    "    # number of training episodes.\n",
    "    # change this to higher number to experiment. say 30000.\n",
    "    number_of_episodes = 10000\n",
    "    max_t = 1000\n",
    "    batchsize = 128\n",
    "    \n",
    "    # amplitude of OU noise\n",
    "    # this slowly decreases to 0\n",
    "    noise = 1\n",
    "    noise_reduction = 0.9999\n",
    "\n",
    "    tau = 1e-3   # soft update factor\n",
    "    gamma = 0.99 # reward discount factor\n",
    "\n",
    "    print_every = 100\n",
    "    # how many episodes before update\n",
    "    episode_per_update = 2\n",
    "\n",
    "    model_dir= os.getcwd()+\"/model_dir\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # do we need to set multi-thread for this env?\n",
    "    torch.set_num_threads(number_of_agents*2)\n",
    "\n",
    "    env = TennisEnv()\n",
    "    \n",
    "    # keep 5000 episodes worth of replay\n",
    "    buffer = ReplayBuffer(int(1e5))\n",
    "    \n",
    "    num_agents, num_states, num_actions = env.get_shapes()\n",
    "\n",
    "    # initialize policy and critic\n",
    "    maddpg = MADDPG(num_agents, num_states, num_actions, discount_factor=gamma, tau=tau)\n",
    "\n",
    "    # training loop\n",
    "    scores_window = deque(maxlen=100)\n",
    "    ep_scores = []\n",
    "\n",
    "\n",
    "    agent0_reward = []\n",
    "    agent1_reward = []\n",
    "\n",
    "    for episode in range(0, number_of_episodes):\n",
    "        reward_this_episode = np.zeros((1, number_of_agents))\n",
    "        states, states_full, env_info = env.reset()\n",
    "\n",
    "        for agent in maddpg.maddpg_agent:\n",
    "            agent.noise.reset()\n",
    "\n",
    "        episode_t = 0\n",
    "\n",
    "        while True:\n",
    "            actions = maddpg.act(torch.tensor(states, dtype=torch.float), noise=noise)\n",
    "\n",
    "            noise *= noise_reduction\n",
    "            actions_for_env = torch.stack(actions).detach().numpy()\n",
    "\n",
    "            # step forward one frame\n",
    "            next_states, next_states_full, rewards, dones, info = env.step(actions_for_env)\n",
    "\n",
    "            # add data to buffer\n",
    "            buffer.push(states, states_full, actions_for_env, rewards, next_states, next_states_full, dones)\n",
    "\n",
    "            reward_this_episode += rewards\n",
    "\n",
    "            states = np.copy(next_states)\n",
    "            states_full = np.copy(next_states_full)\n",
    "\n",
    "            # update once after every episode_per_update\n",
    "            if len(buffer) > batchsize and episode>0 and episode % episode_per_update==0:\n",
    "                for a_i in range(number_of_agents):\n",
    "                    samples = buffer.sample(batchsize)\n",
    "                    maddpg.update(samples, a_i)\n",
    "\n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "            episode_t += 1\n",
    "\n",
    "        agent0_reward.append(reward_this_episode[0, 0])\n",
    "        agent1_reward.append(reward_this_episode[0, 1])\n",
    "        \n",
    "        avg_rewards = max(reward_this_episode[0, 0], reward_this_episode[0, 1])\n",
    "\n",
    "        scores_window.append(avg_rewards)\n",
    "        cur_score = np.mean(scores_window)\n",
    "        ep_scores.append(cur_score)\n",
    "        \n",
    "\n",
    "        save_dict_list =[]\n",
    "\n",
    "        if episode % print_every == 0 or avg_rewards > 2.5:\n",
    "            print('\\rEpisode: {}, Episode Length: {}, Average score: {:.5f}, noise: {:.5f}'.format(episode, episode_t, cur_score, noise))\n",
    "            \n",
    "            if avg_rewards > 2.5:\n",
    "                for i in range(number_of_agents):\n",
    "                    save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                                 'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                                 'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                                 'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}\n",
    "                    save_dict_list.append(save_dict)\n",
    "\n",
    "                    torch.save(save_dict_list, \n",
    "                               os.path.join(model_dir, 'episode-{}.pt'.format(episode)))\n",
    "                print('model saved, exit training')\n",
    "                break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each state: 24\n",
      "Size of each states_full: 48\n",
      "Size of each action: 2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'transpose_to_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ae57cb1f8cf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_tennis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-857a1f855e5f>\u001b[0m in \u001b[0;36mmain_tennis\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaddpg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mnoise_reduction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/codebase/deep-reinforcement-learning-v2/p3_collab-compet/maddpg.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, states_all_agents, noise)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaddpg_agent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates_all_agents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranspose_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transpose_to_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "main_tennis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
